# Characterising-Toxicity-in-Language-Models

## Goal
This project aims to characterize from a linguistic viewpoint what formulations and structures might cause generative large language models (LLMs) to generate toxic content.

## Approach
We use a combination of manual analysis and automated tools to identify patterns in the language generated by LLMs.

## Methodology
1. **Data Collection**: Generate a large corpus of text from various LLMs.
2. **Data Analysis**: Use both manual and automated methods to identify instances of toxic language.
3. **Pattern Identification**: Identify common patterns and structures in the toxic language.

**!!!ATTENTION!!! THE NOTEBOOKS ARE NOT ALL CLEANED AND PROPERLY COMMENTED AND WILL DEFINITELY CONTAIN A NUMBER OF MISTAKES. USE AT YOUR OWN RISK!!!!**

## Structure

### explanations/
This directory contains explanation files in pickle format for different models. These files contain the attributions of model outputs.

### figures/
This directory contains PDF files of the figures generated from the analysis for the report.

### notebooks/
This directory contains Jupyter notebooks used for data analysis and toxicity analysis.

### results/
This directory contains CSV files with the prompt completions for different models. Each file represents the results from a specific model or a set of toxic prompts. It also contains the XLSX file(s) we used for the qualitative evaluation. 
