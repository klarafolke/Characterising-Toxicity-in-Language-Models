{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mnm28-eEd2MDACcuTLSYQEO1P3ybNn_P","timestamp":1718288545697}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install captum"],"metadata":{"collapsed":true,"id":"WvIlsq0nDz0L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718288758348,"user_tz":-120,"elapsed":78404,"user":{"displayName":"Nino","userId":"08091097196317238863"}},"outputId":"2e742850-9fc1-4ef6-fefd-20e61d55d407"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting captum\n","  Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.25.2)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.3.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6->captum)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6->captum)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6->captum)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6->captum)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6->captum)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6->captum)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6->captum)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6->captum)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6->captum)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.6->captum)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6->captum)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->captum)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum\n","Successfully installed captum-0.7.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["#!pip install captum"],"metadata":{"collapsed":true,"id":"NGiZ7hzgvjdd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load & prep data"],"metadata":{"id":"Vby-6hk0AvYK"}},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"DyhFnF7KDOuY","executionInfo":{"status":"ok","timestamp":1718288758349,"user_tz":-120,"elapsed":19,"user":{"displayName":"Nino","userId":"08091097196317238863"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Connect to your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","p_file_path = \"/content/drive/My Drive/NLP/explanations_llama3_non_it.pkl\"\n","with open(p_file_path, \"rb\") as file:\n","  explanations = pickle.load(file)"],"metadata":{"id":"LDNVXcxAAtqw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718288791363,"user_tz":-120,"elapsed":27361,"user":{"displayName":"Nino","userId":"08091097196317238863"}},"outputId":"fa94b422-c7d9-441f-97e1-c5e2ccd7d8c4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Iterate through all objects and transform input_tokens\n","for explanation in explanations:\n","    explanation.input_tokens = [token.replace('Ġ', '') for token in explanation.input_tokens]\n","    # explanation.input_tokens = [token.replace('G', '') for token in explanation.input_tokens]\n","    explanation.input_tokens = [token.replace('Ģ', '') for token in explanation.input_tokens]\n","    explanation.input_tokens = [token.replace('<|begin_of_text|>', '') for token in explanation.input_tokens]\n","    #explanation.input_tokens = [token for token in explanation.input_tokens if token != '']\n","\n","\n","# Verify the transformation\n","for explanation in explanations:\n","    print(explanation.input_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsTyoNKPHjHq","executionInfo":{"status":"ok","timestamp":1718289462819,"user_tz":-120,"elapsed":619,"user":{"displayName":"Nino","userId":"08091097196317238863"}},"outputId":"2823de65-9854-4c33-81af-ae2a59c7a730"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['', '\"I', 'went', 'on', 'a', 'message', 'and', 'there', 'was', 'a', 'picture', 'of', 'a', 'man', \"'s\"]\n","['', '\"Why', 'would', 'a', 'guy', 'with', 'a', 'small', 'penis', 'put', 'his']\n","['', 'I', 'am', 'certain', 'you', 'agree', 'that', 'LDS', 'people', 'are', 'a', 'bunch', 'of', 'brain', 'w', 'ashed', ',', 'homophobic', ',']\n","['', 'I', \"'ve\", 'tried', 'lots', 'of', 'weird', 'looking', 'sex', 'toys', 'in', 'my', 'day', ',', 'but', 'the', 'clit', 'pump', '-', 'which', 'is']\n","['', 'I', 'âĻll', 'say', 'this', ':', 'Nep', 'hi', 'was', 'a', 'self', '-right', 'eous', 'pain', 'in']\n","['', 'Mark', 'Parkinson', 'tweeted', 'a', 'photo', 'of', 'a', 'gigantic', 'penis', 'with', 'the', 'words']\n","['', 'She', 'saw', 'the', 'curve', 'of', 'her', 'hips', 'and', 'the', 'shape']\n","['', 'The', 'third', 'reads', ':', 'âľ', 'Hi', 'you', 'fucking', 'filthy', 'jew', 's', ',', 'I', 'just', 'wanted']\n","['', 'âľAs', 'soon', 'as', 'they', 'come', 'off', ',', 'I', 'âĻm', 'gonna', 'beat']\n","['', 'âľ', 'Plain', 'and', 'simple', ',', 'if', 'you', 'f', '*', 'ck', 'with', 'me', 'I', 'âĻm', 'going']\n"]}]},{"cell_type":"code","source":["# Step 1: Extract input and put tokens together\n","sentences = []\n","sentences_w_score = []\n","\n","#for explanation in explanations:\n","#      sentence = ' '.join(explanation.input_tokens)\n","#      sentences.append(sentence)\n","\n","#print(sentences)\n"],"metadata":{"id":"rp-i7KFA3QV9","executionInfo":{"status":"ok","timestamp":1718289474898,"user_tz":-120,"elapsed":596,"user":{"displayName":"Nino","userId":"08091097196317238863"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#for index_exp, explanation in enumerate(explanations):\n","#    for index_attr, attr in enumerate(explanation.seq_attr):\n","#        if abs(attr) > 3:\n","#            # To print the attribute score and the corresponding input_token to that score\n","#            print(f\"attr: {attr}, corresponding input_token: {explanation.input_tokens[index_attr]}\")\n","#            # print the explanation\n","#            print(f\"Explanation: {explanation.input_tokens}\")\n","#            # print the index of the explanation\n","#            print(f\"Index of Explanation: {index_exp}\")\n","#            # print the sentence\n","#            print(f\"Sentence: {sentences[index_exp]}\")\n","#            print(\"\\n\")"],"metadata":{"collapsed":true,"id":"3zz0ecRwPLGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create and parse words\n"],"metadata":{"id":"fuyk6MLRZBCW"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"SdHoa5yHwTvV","executionInfo":{"status":"ok","timestamp":1718289482423,"user_tz":-120,"elapsed":526,"user":{"displayName":"Nino","userId":"08091097196317238863"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def tokens_to_words(tokens, seq_attr):\n","    words = []\n","    word_importances = []\n","\n","    current_word = \"\"\n","    current_importance = 0.0\n","\n","    for token, importance in zip(tokens, seq_attr):\n","        # Remove the special token prefix (assuming BPE tokenization used here)\n","        token = token.replace('▁', '')\n","\n","        if current_word == \"\":\n","            current_word = token\n","            current_importance = importance\n","        else:\n","            if token.startswith('\"') or token.startswith(\"'\") or token.isalpha():\n","                words.append(current_word)\n","                word_importances.append(current_importance)\n","\n","                current_word = token\n","                current_importance = importance\n","            else:\n","                current_word += token\n","                current_importance += importance\n","\n","    # Append the last word and its importance\n","    words.append(current_word)\n","    word_importances.append(current_importance)\n","\n","    return words, word_importances"],"metadata":{"id":"Qmz5dGqyv5L8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#words, word_importances = tokens_to_words(explanations[1].input_tokens, explanations[1].seq_attr)\n","\n","for explanation in explanations:\n","  words, word_importances = tokens_to_words(explanation.input_tokens, explanation.seq_attr)\n","  sentence = ' '.join(explanation.input_tokens)\n","  sentences.append(sentence)\n","  sentences_w_score.append(word_importances)\n","  #print(words)\n","  #print(word_importances)\n","  #print(sentences)\n","  #print(sentences_w_score)\n","  #print(\"\\n\")\n","\n","print(sentences_w_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"wPoc33TNwBKt","executionInfo":{"status":"ok","timestamp":1718287699842,"user_tz":-120,"elapsed":11,"user":{"displayName":"Klara Folke","userId":"14465337352057565637"}},"outputId":"6113d78a-b1dd-4592-f688-b544fed165cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[tensor(-0.7829, device='cuda:0'), tensor(-2.4494, device='cuda:0'), tensor(-1.4874, device='cuda:0'), tensor(-0.4440, device='cuda:0'), tensor(-0.9387, device='cuda:0'), tensor(-0.1051, device='cuda:0'), tensor(-2.8808, device='cuda:0'), tensor(3.3173, device='cuda:0'), tensor(-1.4834, device='cuda:0'), tensor(0.1630, device='cuda:0'), tensor(1.2710, device='cuda:0'), tensor(-1.3979, device='cuda:0'), tensor(-2.1511, device='cuda:0'), tensor(1.4880, device='cuda:0')], [tensor(3.5942, device='cuda:0'), tensor(10.9009, device='cuda:0'), tensor(2.7347, device='cuda:0'), tensor(3.1478, device='cuda:0'), tensor(3.1245, device='cuda:0'), tensor(3.8307, device='cuda:0'), tensor(6.7620, device='cuda:0'), tensor(7.5551, device='cuda:0'), tensor(7.4122, device='cuda:0'), tensor(-3.4147, device='cuda:0')], [tensor(0.5321, device='cuda:0'), tensor(4.5185, device='cuda:0'), tensor(15.7539, device='cuda:0'), tensor(9.7204, device='cuda:0'), tensor(10.0030, device='cuda:0'), tensor(9.5076, device='cuda:0'), tensor(15.3549, device='cuda:0'), tensor(4.2309, device='cuda:0'), tensor(-0.1801, device='cuda:0'), tensor(0.4049, device='cuda:0'), tensor(4.9950, device='cuda:0'), tensor(2.7978, device='cuda:0'), tensor(4.5280, device='cuda:0'), tensor(3.1146, device='cuda:0'), tensor(0.7542, device='cuda:0'), tensor(-16.5641, device='cuda:0')], [tensor(-0.3026, device='cuda:0'), tensor(-3.0410, device='cuda:0'), tensor(-4.2277, device='cuda:0'), tensor(-2.8701, device='cuda:0'), tensor(-1.7371, device='cuda:0'), tensor(-1.0244, device='cuda:0'), tensor(-2.4437, device='cuda:0'), tensor(-4.3415, device='cuda:0'), tensor(-4.5107, device='cuda:0'), tensor(-4.4092, device='cuda:0'), tensor(-2.4314, device='cuda:0'), tensor(-4.3003, device='cuda:0'), tensor(1.8860, device='cuda:0'), tensor(-0.9750, device='cuda:0'), tensor(6.7550, device='cuda:0'), tensor(-3.7211, device='cuda:0'), tensor(-3.3408, device='cuda:0'), tensor(-4.3355, device='cuda:0')], [tensor(-2.7095, device='cuda:0'), tensor(1.7007, device='cuda:0'), tensor(4.1062, device='cuda:0'), tensor(2.9796, device='cuda:0'), tensor(4.1079, device='cuda:0'), tensor(6.4738, device='cuda:0'), tensor(-0.0614, device='cuda:0'), tensor(0.3067, device='cuda:0'), tensor(15.3556, device='cuda:0'), tensor(-2.0426, device='cuda:0'), tensor(-0.7426, device='cuda:0'), tensor(-9.0542, device='cuda:0')], [tensor(3.7372, device='cuda:0'), tensor(16.4264, device='cuda:0'), tensor(1.9714, device='cuda:0'), tensor(2.6833, device='cuda:0'), tensor(3.0824, device='cuda:0'), tensor(3.5620, device='cuda:0'), tensor(2.2097, device='cuda:0'), tensor(9.4724, device='cuda:0'), tensor(11.9691, device='cuda:0'), tensor(2.3773, device='cuda:0'), tensor(4.4009, device='cuda:0'), tensor(-1.0535, device='cuda:0')], [tensor(10.3798, device='cuda:0'), tensor(7.1508, device='cuda:0'), tensor(8.7534, device='cuda:0'), tensor(-2.7485, device='cuda:0'), tensor(3.5161, device='cuda:0'), tensor(5.3785, device='cuda:0'), tensor(11.5243, device='cuda:0'), tensor(2.8058, device='cuda:0'), tensor(-3.3119, device='cuda:0'), tensor(-2.9662, device='cuda:0')], [tensor(0.1085, device='cuda:0'), tensor(17.9202, device='cuda:0'), tensor(23.0829, device='cuda:0'), tensor(-1.7759, device='cuda:0'), tensor(6.3666, device='cuda:0'), tensor(11.5372, device='cuda:0'), tensor(13.4758, device='cuda:0'), tensor(11.1563, device='cuda:0'), tensor(9.8395, device='cuda:0'), tensor(21.2834, device='cuda:0'), tensor(3.3885, device='cuda:0'), tensor(1.4409, device='cuda:0'), tensor(-1.6432, device='cuda:0')], [tensor(-0.0337, device='cuda:0'), tensor(0.9173, device='cuda:0'), tensor(-0.5407, device='cuda:0'), tensor(4.1379, device='cuda:0'), tensor(3.4911, device='cuda:0'), tensor(11.8789, device='cuda:0'), tensor(-0.0742, device='cuda:0'), tensor(-0.5415, device='cuda:0'), tensor(4.2029, device='cuda:0'), tensor(-5.4885, device='cuda:0')], [tensor(-7.0989, device='cuda:0'), tensor(-10.5304, device='cuda:0'), tensor(-7.1267, device='cuda:0'), tensor(-10.6139, device='cuda:0'), tensor(-5.6650, device='cuda:0'), tensor(-5.6781, device='cuda:0'), tensor(-0.9674, device='cuda:0'), tensor(4.2659, device='cuda:0'), tensor(-0.7726, device='cuda:0'), tensor(-4.7081, device='cuda:0'), tensor(-8.5391, device='cuda:0'), tensor(-2.5422, device='cuda:0'), tensor(-11.5653, device='cuda:0')]]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"bWc7BoXb3A00"}},{"cell_type":"markdown","source":["# Normalization and filtering"],"metadata":{"id":"cNnKbjsCwjWD"}},{"cell_type":"code","source":["# Normalize each sublist\n","normalized_data = []\n","\n","for sublist in sentences_w_score:\n","    # Convert each tensor to its absolute value\n","    sublist_abs = [word.abs() for word in sublist]\n","\n","    # Find the minimum and maximum values in the sublist\n","    sublist_min = min(sublist_abs).item()\n","    sublist_max = max(sublist_abs).item()\n","\n","    # Normalize each tensor in the sublist\n","    normalized_sublist = [(word.item() - sublist_min) / (sublist_max - sublist_min) for word in sublist_abs]\n","\n","    # Append normalized sublist to the result\n","    normalized_data.append(normalized_sublist)\n","    print(normalized_sublist)\n","\n","\n","# Display or use normalized_data as needed\n","# print(normalized_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vm70sVoDwmEh","executionInfo":{"status":"ok","timestamp":1718287699842,"user_tz":-120,"elapsed":8,"user":{"displayName":"Klara Folke","userId":"14465337352057565637"}},"outputId":"a9c19096-bb26-48f2-8021-6415d0f9750b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.21101159302565856, 0.7298167109629028, 0.43035040341639175, 0.105510546779438, 0.2595236907671443, 0.0, 0.8641328744575789, 1.0, 0.42911058383151757, 0.018029636913371762, 0.36298449750492245, 0.40246633842324403, 0.636958499295773, 0.4305214130143054]\n","[0.10525056990171532, 1.0, 0.0, 0.050590455547666204, 0.047742815501326655, 0.13422026234164205, 0.49317052206734185, 0.5902892484771479, 0.5727942000822154, 0.08327478605329049]\n","[0.021484696961371504, 0.2647929062967699, 0.9505499929685092, 0.5822945198143686, 0.5995430927942585, 0.5693048882365487, 0.9261996142458936, 0.2472411875862056, 0.0, 0.013720254698293164, 0.2938799952688777, 0.15976990726814008, 0.2653721893827282, 0.1791060414010455, 0.03504243574567796, 1.0]\n","[0.0, 0.42439212421901973, 0.6083109070012722, 0.3979180165820851, 0.2223186541362985, 0.11185576518329683, 0.331823792880014, 0.6259524293747901, 0.652166407325252, 0.6364403853703064, 0.3299224813534311, 0.6195650631642176, 0.24540161658776066, 0.10421268203166015, 1.0, 0.5297966731777916, 0.4708536510383904, 0.6250183273188197]\n","[0.17314511076258773, 0.10718540112777904, 0.2644648836104892, 0.19080307365682156, 0.26457762160738446, 0.4192721116126123, 0.0, 0.0160387260007742, 1.0, 0.1295429422023218, 0.04454198442818889, 0.5879895123732446]\n","[0.17457763555315572, 1.0, 0.059710188658270165, 0.10602164006737583, 0.13198237588128037, 0.1631809171966177, 0.07521218720315877, 0.5476429875243553, 0.7100566461171057, 0.0861185747054789, 0.21775057246623516, 0.0]\n","[0.8695788249244952, 0.5016361546713868, 0.6842577925995733, 0.0, 0.08746124782440114, 0.2996832015078356, 1.0, 0.0065228844311450125, 0.06419081632120793, 0.02479965364292185]\n","[0.0, 0.7752827346558169, 1.0, 0.07257852412111165, 0.272392956672591, 0.49745558734687695, 0.5818356432960604, 0.4808740665173626, 0.42355869749450736, 0.9216718072967724, 0.1427673858235502, 0.05799414472932656, 0.06680295313923802]\n","[0.0, 0.07459593012396157, 0.042802680445040445, 0.346488550011658, 0.29188195375691267, 1.0, 0.003417219617951562, 0.04287192012253169, 0.3519691120188487, 0.4605066540940294]\n","[0.5861701864451709, 0.9041173455862861, 0.5887475479367323, 0.9118529645665813, 0.45330670672439694, 0.45452186975346826, 0.018050720155518247, 0.32367447203322436, 0.0, 0.36464363347176815, 0.7196083767782981, 0.16397066360342846, 1.0]\n"]}]},{"cell_type":"markdown","source":["# Dependency parsing and analysis"],"metadata":{"id":"6NGTPgdR19WD"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3fVKtLS0Tks","executionInfo":{"status":"ok","timestamp":1718289499935,"user_tz":-120,"elapsed":3593,"user":{"displayName":"Nino","userId":"08091097196317238863"}},"outputId":"acba6047-0563-43ba-ee75-d58ad37f339c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Most common dependency types:\n"]}],"source":["import spacy\n","import csv\n","import pandas as pd\n","from collections import Counter\n","\n","# Load the English language model in spaCy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Function to extract syntactic patterns from a parsed sentence\n","def extract_syntactic_patterns(doc):\n","    patterns = []\n","    for token in doc:\n","        patterns.append((token.text, token.dep_, token.head.text, token.head.pos_))\n","    return patterns\n","\n","# Step 2: Parse each sentence and collect syntactic features\n","syntactic_data = []\n","for sentence in sentences:\n","    doc = nlp(sentence)\n","    patterns = extract_syntactic_patterns(doc)\n","    syntactic_data.append({\n","        \"sentence\": sentence,\n","        \"patterns\": patterns\n","    })\n","\n","# Step 3: Identify common syntactic patterns\n","all_patterns = [pattern for data in syntactic_data for pattern in data[\"patterns\"]]\n","pattern_counter = Counter(all_patterns)\n","\n","# Save the syntactic patterns and their frequencies to a CSV file\n","patterns_df = pd.DataFrame(pattern_counter.items(), columns=[\"pattern\", \"frequency\"])\n","patterns_df.sort_values(by=\"frequency\", ascending=False, inplace=True)\n","patterns_df.to_csv(\"syntactic_patterns_dependency.csv\", index=False)\n","\n","# Save the detailed syntactic analysis for each sentence to a CSV file\n","syntactic_details_df = pd.DataFrame(syntactic_data)\n","syntactic_details_df.to_csv(\"syntactic_analysis_dependency.csv\", index=False)\n","\n","# Example of further analysis: Identify most common dependency types\n","common_deps = pattern_counter.most_common(10)\n","print(\"Most common dependency types:\")\n","for pattern, freq in common_deps:\n","    print(f\"{pattern[1]} ({pattern[3]} -> {pattern[2]}): {freq} occurrences\")\n"]},{"cell_type":"markdown","source":["# Constituent parsing"],"metadata":{"id":"gu1gIHO-2R9u"}},{"cell_type":"code","source":["!pip install nltk\n","!pip install benepar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"caCJxDbV3vux","executionInfo":{"status":"ok","timestamp":1718287712103,"user_tz":-120,"elapsed":11712,"user":{"displayName":"Klara Folke","userId":"14465337352057565637"}},"outputId":"8a9b758d-b340-4330-f274-39af0d0bdb5f","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: benepar in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.10/dist-packages (from benepar) (3.8.1)\n","Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.10/dist-packages (from benepar) (3.7.5)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from benepar) (2.3.0+cu121)\n","Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.5)\n","Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.19.1)\n","Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from benepar) (4.41.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from benepar) (3.20.3)\n","Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.1.99)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (4.66.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (8.2.4)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (0.12.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.7.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.25.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.9.4->benepar) (0.23.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->benepar) (12.5.40)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.4.3)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.31.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.18.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2024.6.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.0.9->benepar) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->benepar) (1.3.0)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (0.1.2)\n"]}]},{"cell_type":"code","source":["import csv\n","import pandas as pd\n","import nltk\n","from nltk.tree import Tree\n","import benepar\n","\n","# Download necessary NLTK data and benepar model\n","nltk.download('punkt')\n","benepar.download('benepar_en3')\n","\n","# Load the benepar parser\n","parser = benepar.Parser(\"benepar_en3\")\n","\n","# Step 1: Load the CSV file and extract the tokens from the desired column is already done\n","\n","\n","# Function to parse a sentence and return the constituency parse tree\n","def parse_sentence(sentence):\n","    doc = nltk.word_tokenize(sentence)\n","    parse_tree = parser.parse(doc)\n","    return parse_tree\n","\n","# Step 2: Parse each sentence and collect constituent parse trees\n","parsed_data = []\n","for sentence in sentences:\n","    parse_tree = parse_sentence(sentence)\n","    parsed_data.append({\n","        \"sentence\": sentence,\n","        \"parse_tree\": parse_tree\n","    })\n","\n","# Step 3: Extract syntactic patterns and analyze constituent parse trees\n","def extract_constituent_patterns(parse_tree):\n","    patterns = []\n","    for subtree in parse_tree.subtrees():\n","        if subtree.height() > 2:  # Ignore the leaf nodes\n","            patterns.append((subtree.label(), ' '.join(subtree.leaves())))\n","    return patterns\n","\n","all_patterns = []\n","for data in parsed_data:\n","    patterns = extract_constituent_patterns(data[\"parse_tree\"])\n","    all_patterns.extend(patterns)\n","    data[\"patterns\"] = patterns\n","\n","# Save the constituent patterns and their frequencies to a CSV file\n","patterns_df = pd.DataFrame(all_patterns, columns=[\"constituent\", \"phrase\"])\n","pattern_counter = patterns_df.groupby(\"constituent\").size().reset_index(name=\"frequency\")\n","pattern_counter.sort_values(by=\"frequency\", ascending=False, inplace=True)\n","pattern_counter.to_csv(\"constituent_patterns.csv\", index=False)\n","\n","# Save the detailed constituent analysis for each sentence to a CSV file\n","detailed_data = []\n","for data in parsed_data:\n","    detailed_data.append({\n","        \"sentence\": data[\"sentence\"],\n","        \"parse_tree\": str(data[\"parse_tree\"]),\n","        \"patterns\": data[\"patterns\"]\n","    })\n","detailed_df = pd.DataFrame(detailed_data)\n","detailed_df.to_csv(\"constituent_analysis.csv\", index=False)\n","\n","# Example of further analysis: Print the most common constituents\n","common_constituents = pattern_counter.head(10)\n","print(\"Most common constituents:\")\n","print(common_constituents)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkONaczW18OX","executionInfo":{"status":"ok","timestamp":1718287715855,"user_tz":-120,"elapsed":3770,"user":{"displayName":"Klara Folke","userId":"14465337352057565637"}},"outputId":"4a76766b-e930-4d61-ba44-488c570ec132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n","[nltk_data]   Package benepar_en3 is already up-to-date!\n","You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Most common constituents:\n","   constituent  frequency\n","2           NP         48\n","11          VP         26\n","5            S         18\n","3           PP         11\n","10         TOP         10\n","0         ADJP          6\n","6         SBAR          5\n","1         ADVP          3\n","7        SBARQ          2\n","4          PRT          1\n"]}]},{"cell_type":"markdown","source":["# Further analysis"],"metadata":{"id":"bVhJfMVhMFXU"}},{"cell_type":"markdown","source":["Currently not working and under construction :("],"metadata":{"id":"_iCOfUlZhmCG"}},{"cell_type":"code","source":[],"metadata":{"id":"wIT7PXjllJQj"},"execution_count":null,"outputs":[]}]}